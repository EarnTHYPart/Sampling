# -*- coding: utf-8 -*-
"""Sampling Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DNw_oNnNxdg0e7xk4d7t3AafDVJzFSkl
"""

!pip install imbalanced-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Sampling
from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler, NearMiss

df = pd.read_csv('/content/Creditcard_data.csv')
df.head()

sns.countplot(x='Class', data=df)
plt.title("Original Class Distribution")
plt.show()

plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

X = df.drop('Class', axis=1)
y = df['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

sampling_methods = {
    "RandomOver": RandomOverSampler(),
    "SMOTE": SMOTE(),
    "ADASYN": ADASYN(),
    "RandomUnder": RandomUnderSampler(),
    "NearMiss": NearMiss()
}

models = {
    "Logistic": LogisticRegression(max_iter=1000),
    "DecisionTree": DecisionTreeClassifier(),
    "RandomForest": RandomForestClassifier(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier()
}

results = pd.DataFrame(columns=["Model","Sampling","Accuracy"])

for samp_name, sampler in sampling_methods.items():
    X_res, y_res = sampler.fit_resample(X_scaled, y)

    X_train, X_test, y_train, y_test = train_test_split(
        X_res, y_res, test_size=0.2, random_state=42
    )

    for model_name, model in models.items():
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        acc = accuracy_score(y_test, preds)

        results.loc[len(results)] = [model_name, samp_name, acc]

pivot_table = results.pivot(index="Model", columns="Sampling", values="Accuracy")
pivot_table

plt.figure(figsize=(8,6))
sns.heatmap(pivot_table, annot=True, cmap="viridis")
plt.title("Accuracy Heatmap")
plt.show()

results.plot(kind='bar', x='Model', y='Accuracy', figsize=(12,6))
plt.title("Model Accuracy Comparison")
plt.show()

best_sampling = results.loc[results.groupby('Model')['Accuracy'].idxmax()]
best_sampling

best_model_name = best_sampling.iloc[0]['Model']
best_sampling_name = best_sampling.iloc[0]['Sampling']

best_model = models[best_model_name]
best_sampler = sampling_methods[best_sampling_name]

X_res, y_res = best_sampler.fit_resample(X_scaled, y)
best_model.fit(X_res, y_res)

def predict_sample(sample):
    sample_scaled = scaler.transform([sample])
    return best_model.predict(sample_scaled)

plt.figure(figsize=(10,6))

for model in results['Model'].unique():
    subset = results[results['Model'] == model]
    plt.plot(subset['Sampling'], subset['Accuracy'], marker='o', label=model)

plt.title("Accuracy Comparison Across Sampling Techniques")
plt.xlabel("Sampling Technique")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

avg_sampling = results.groupby('Sampling')['Accuracy'].mean().sort_values(ascending=False)

plt.figure(figsize=(8,5))
avg_sampling.plot(kind='bar')
plt.title("Average Accuracy per Sampling Technique")
plt.ylabel("Accuracy")
plt.xlabel("Sampling Technique")
plt.show()

best_sampling = results.loc[results.groupby('Model')['Accuracy'].idxmax()]

plt.figure(figsize=(8,5))
plt.bar(best_sampling['Model'], best_sampling['Accuracy'])
plt.title("Best Sampling Technique per Model")
plt.ylabel("Accuracy")
plt.xlabel("Model")
plt.show()

print(best_sampling)

ranking = avg_sampling.rank(ascending=False)

plt.figure(figsize=(8,5))
plt.bar(ranking.index, ranking.values)
plt.title("Sampling Technique Ranking")
plt.ylabel("Rank (Lower is Better)")
plt.xlabel("Sampling Technique")
plt.gca().invert_yaxis()
plt.show()

variance_sampling = results.groupby('Sampling')['Accuracy'].var()

plt.figure(figsize=(8,5))
variance_sampling.plot(kind='bar')
plt.title("Sampling Technique Stability (Variance)")
plt.ylabel("Variance")
plt.xlabel("Sampling Technique")
plt.show()

pivot = results.pivot(index='Model', columns='Sampling', values='Accuracy')

pivot.plot(kind='bar', figsize=(12,6))
plt.title("Accuracy Comparison: Models vs Sampling")
plt.ylabel("Accuracy")
plt.xlabel("Model")
plt.legend(title="Sampling")
plt.show()

!pip install xgboost

from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier

models = {
    "Logistic": LogisticRegression(max_iter=1000),
    "DecisionTree": DecisionTreeClassifier(),
    "RandomForest": RandomForestClassifier(),
    "SVM": SVC(probability=True),
    "KNN": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "NaiveBayes": GaussianNB(),
    "GradientBoost": GradientBoostingClassifier()
}

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import time

results = []

for samp_name, sampler in sampling_methods.items():
    X_res, y_res = sampler.fit_resample(X_scaled, y)

    X_train, X_test, y_train, y_test = train_test_split(
        X_res, y_res, test_size=0.2, random_state=42
    )

    for model_name, model in models.items():
        start = time.time()
        model.fit(X_train, y_train)
        end = time.time()

        preds = model.predict(X_test)

        if hasattr(model, "predict_proba"):
            probs = model.predict_proba(X_test)[:,1]
            roc = roc_auc_score(y_test, probs)
        else:
            roc = 0

        results.append([
            model_name,
            samp_name,
            accuracy_score(y_test, preds),
            precision_score(y_test, preds),
            recall_score(y_test, preds),
            f1_score(y_test, preds),
            roc,
            end-start
        ])

results_df = pd.DataFrame(results, columns=[
    "Model","Sampling","Accuracy","Precision","Recall","F1","ROC_AUC","Time"
])

results_df.head()

pivot_acc = results_df.pivot(index="Model", columns="Sampling", values="Accuracy")

pivot_acc.plot(kind='bar', figsize=(14,6))
plt.title("Accuracy Comparison")
plt.ylabel("Accuracy")
plt.show()

pivot_f1 = results_df.pivot(index="Model", columns="Sampling", values="F1")

pivot_f1.plot(kind='bar', figsize=(14,6))
plt.title("F1 Score Comparison")
plt.ylabel("F1 Score")
plt.show()

sns.boxplot(x="Sampling", y="Accuracy", data=results_df)
plt.title("Accuracy Distribution by Sampling")
plt.show()

time_avg = results_df.groupby("Model")["Time"].mean()

time_avg.plot(kind='bar')
plt.title("Average Training Time per Model")
plt.ylabel("Seconds")
plt.show()

from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(8,6))

for name, model in models.items():
    model.fit(X_train, y_train)
    probs = model.predict_proba(X_test)[:,1]
    fpr, tpr, _ = roc_curve(y_test, probs)
    plt.plot(fpr, tpr, label=f"{name} AUC={auc(fpr,tpr):.2f}")

plt.plot([0,1],[0,1],'--')
plt.legend()
plt.title("ROC Curves")
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

best = results_df.loc[results_df["Accuracy"].idxmax()]
best_model = models[best["Model"]]
best_sampler = sampling_methods[best["Sampling"]]

X_res, y_res = best_sampler.fit_resample(X_scaled, y)
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res)

best_model.fit(X_train, y_train)
preds = best_model.predict(X_test)

cm = confusion_matrix(y_test, preds)
ConfusionMatrixDisplay(cm).plot()
plt.title("Confusion Matrix - Best Model")
plt.show()

rf = RandomForestClassifier()
rf.fit(X_train, y_train)

plt.bar(range(len(rf.feature_importances_)), rf.feature_importances_)
plt.title("Feature Importance - Random Forest")
plt.show()

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(RandomForestClassifier(), X_scaled, y, cv=5)
print("Cross Validation Accuracy:", cv_scores.mean())

avg_sampling = results_df.groupby("Sampling")["Accuracy"].mean()
avg_sampling.plot(kind='bar')
plt.title("Average Accuracy per Sampling")
plt.show()

import os
import matplotlib.pyplot as plt
import seaborn as sns
from zipfile import ZipFile

# 1. CREATE FOLDER
folder_path = "plots"
os.makedirs(folder_path, exist_ok=True)

# 2. SAVE FUNCTION
def save_plot(name):
    plt.savefig(f"{folder_path}/{name}.png", dpi=300, bbox_inches='tight')
    plt.close()

# 3. ACCURACY BAR
pivot_acc.plot(kind='bar', figsize=(14,6))
plt.title("Accuracy Comparison")
plt.ylabel("Accuracy")
save_plot("accuracy_bar")

# 4. F1 SCORE BAR
pivot_f1.plot(kind='bar', figsize=(14,6))
plt.title("F1 Score Comparison")
plt.ylabel("F1 Score")
save_plot("f1_bar")

# 5. BOX PLOT
plt.figure(figsize=(8,6))
sns.boxplot(x="Sampling", y="Accuracy", data=results_df)
plt.title("Sampling Stability")
save_plot("sampling_boxplot")

# 6. ROC CURVE
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr)
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
save_plot("roc_curve")

# 7. CONFUSION MATRIX
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
save_plot("confusion_matrix")

# 8. FEATURE IMPORTANCE
plt.figure(figsize=(10,5))
plt.bar(range(len(rf.feature_importances_)), rf.feature_importances_)
plt.title("Feature Importance")
save_plot("feature_importance")

# 9. AVERAGE SAMPLING ACCURACY
avg_sampling.plot(kind='bar', figsize=(8,5))
plt.title("Average Sampling Accuracy")
save_plot("avg_sampling_accuracy")

# 10. TRAINING TIME
time_avg.plot(kind='bar', figsize=(8,5))
plt.title("Training Time per Model")
save_plot("training_time")

# 11. ZIP ALL IMAGES FOR DOWNLOAD
zip_name = "plots.zip"
with ZipFile(zip_name, 'w') as zipf:
    for file in os.listdir(folder_path):
        zipf.write(os.path.join(folder_path, file))

print("All plots saved and zipped as plots.zip")